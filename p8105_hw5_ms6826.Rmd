---
title: "p8105_hw5_ms6826"
output: github_document
date: "2024-11-09"
---

```{r}
library(tidyverse)
set.seed(1031)
```


## Problem 1

Write a function
```{r}
bday_sim= function(n) {
  
  bdays= sample(1:365, size=n, replace = TRUE)

  duplicate= length(unique(bdays))<n
  
  return(duplicate)
  
}
```

Run this function 10000 times, compute probability, and make a plot showing the probability as a function of group size
```{r}
sim_res=
  expand.grid(
    n = 2:50,
    iter= 1:1000
  ) |> 
  mutate(res=map_lgl(n, bday_sim)) |> 
  group_by(n) |> 
  summarize(prob=mean(res))

sim_res |> 
  ggplot(aes(x= n, y = prob)) +
  geom_line()
```
Comment: As group size increases, the probability of duplicate birthdays also increases.

## Problem 2
Set the design elements and generate datasets 
```{r}
sim_power= function(samp_size=30, true_mean=0, true_sd=5, alpha=0.05) {
  sim_df=
    tibble(
      x=rnorm(samp_size, true_mean,true_sd)
    )
  
   out_df=
    sim_df |> 
    summarize(
      mu_hat=mean(x),
      p_value=t.test(x, mu=0) |> broom::tidy() |> 
     pull(p.value)
    ) 
    
    
   return(out_df)
  
}

sim_results_df=
  expand_grid(
    iter= 1:5000
  ) |> 
  mutate(power_df=map(iter, sim_power, samp_size=30, true_mean=0)
         ) |> 
  unnest(power_df)
print(sim_results_df)
```

Repeat the above for 𝜇={1,2,3,4,5,6}
```{r}
sim_final=
  expand_grid(
    true_mean=c(0,1,2,3,4,5,6),
    iter=1:5000
  ) |> 
  mutate(samp_res=map(true_mean, sim_power, samp_size=30)) |> 
  unnest(samp_res)
print(sim_final)
```

Make a plot showing power vs true mean
```{r}
sim_final |> 
  group_by(true_mean) |> 
  summarize(power=mean(p_value<0.05)) |> 
  ggplot(aes(x=true_mean, y=power)) +
  geom_point() +
  geom_line() +
  labs(
    title = "Power vs. True Value of μ",
    x = "True Value of μ",
    y = "Power"
  ) +
  theme_minimal()
```
Comment: As effect size increases, the power also increases. 

Make a plot for the average estimate of 𝜇̂ on the y axis and the true value of 𝜇on the x axis.
```{r}
sim_final |> 
  group_by(true_mean) |> 
  summarize(avg_mu_hat=mean(mu_hat), .groups = 'drop') |> 
  ggplot(aes(x=true_mean, y=avg_mu_hat))+
  geom_point() +
  geom_line() +
  labs(
    title = "Average Estimate of μ hat vs True Value of μ",
    x = "True Value of μ",
    y = "Average Estimate of μ hat"
  ) +
  theme_minimal()
```

Make a second plot only in samples for which the null was rejected 
```{r}
sim_final |> 
  filter(p_value<0.05) |> 
  group_by(true_mean) |> 
  summarize(avg_mu_hat_rejected=mean(mu_hat), .groups = 'drop') |> 
  ggplot(aes(x=true_mean, y=avg_mu_hat_rejected))+
  geom_point() +
  geom_line() +
  labs(
    title = "Average Estimate of μ hat When the Null was Rejected",
    x = "True Value of μ",
    y = "Average Estimate of μ hat"
  ) +
  theme_minimal()

```
--Yes, the sample average of mu hat across tests for which the null is rejected is approximately equal to the true value of mu (especially for larger values of mu), and this is expected because a sample where the statistical test is  rejected means that the estimated mu was significantly different (with the significance level=0.05) from zero.


## Problem 3
Describe the raw data: The raw data for each homicide includes an ID# that lists the first three letters of the city the homicide took place as well as its # on this list of 52,169 homicides.The raw data also includes the reported date of the homicide, the location of the homicide (city, state, lat, long), demographic information on the victim (e.g., first name, last name, race, age, etc.), as well as the disposition.

Create a city_state variable and summarize within cities to obtain the total number of homicides and the number of unsolved homicides.
```{r}
homicide_df=
  read_csv("data/homicide-data.csv") 

hom_df=
homicide_df |> 
  mutate(city_state=paste(city, state, sep=", ")) |> 
  group_by(city_state) |> 
  summarize(
    tot_homicides=n(),
    unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  )
```


For the city of Baltimore, MD, use the prop.test function and pull the estimated proportion and confidence intervals from the resulting tidy dataframe
```{r}
balt_df=
  prop.test(
    x=hom_df |> filter(city_state=="Baltimore, MD") |> pull(unsolved_homicides), 
    n=hom_df |> filter(city_state=="Baltimore, MD") |> pull(tot_homicides)) |> 
  broom::tidy()

balt_df |> 
  knitr::kable() 

balt_df |> 
  pull(estimate) 
balt_df |> 
  pull(conf.low)
balt_df |> 
  pull(conf.high)
```

Run prop.test for each of the cities in your dataset and create a tidy dataframe with estimated proportions and CIs for each city.
```{r}
results_df=
  hom_df |> 
  mutate(
    prop_table=map2(unsolved_homicides, tot_homicides, ~prop.test(x=.x, n=.y)),
    tidy=map(prop_table, broom::tidy)) |> 
  select(-prop_table) |> 
  unnest(cols = c(tidy)) |> 
  select(city_state, estimate, conf.low, conf.high) 

print(results_df)

```


Create a plot that shows the estimates and CIs for each city
```{r}
results_df |> 
  arrange(estimate) |> 
  mutate(city_state = factor(city_state, levels = city_state)) |> 
  ggplot(aes(x=city_state, y=estimate))+
  geom_point()+
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2)+
  coord_flip()+
    labs(
    title = "Proportion of Unsolved Homicides by City",
    x = "City",
    y = "Estimated Proportion (with 95% CI)"
  )

```


